{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigrams, Bigrams, and Trigrams in Naive Bayes Classifiers\n",
    "\n",
    "Math of Intelligence Week 6 Challenge - https://www.youtube.com/watch?v=PrkiRVcrxOs&t=7s\n",
    "\n",
    "In this notebook I will explore the performance of ngram words in a naive bayes classifier. I will look at how they perform across two data sets: \n",
    "    1) A Spam SMS dataset \n",
    "    2) Rap lines from Biggie Smalls and 2Pac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./spamSMS.csv', usecols=[0,1], encoding='latin-1')\n",
    "df.columns = ['label','body']\n",
    "# label spam as 1, not spam as 0\n",
    "df['label'] = df['label'].replace([\"ham\",\"spam\"],[0,1])\n",
    "data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ngrams_bayes():\n",
    "    \n",
    "    def __init__(self, data, n=2, split=0.75):\n",
    "        \n",
    "        # split into training and testing data\n",
    "        self.train_data, self.test_data = train_test_split(data,\n",
    "                                                          train_size=split)\n",
    "        # convert into n grams\n",
    "        self.train_data = [[item[0], self.ngrams(n, item[1])] for item in self.train_data]\n",
    "        self.test_data = [[item[0], self.ngrams(n, item[1])] for item in self.test_data]\n",
    "        \n",
    "        # count unique n grams in training data\n",
    "        flattened = [gram for message in self.train_data for gram in message[1]]\n",
    "        self.unique = len(set(flattened))\n",
    "        \n",
    "        # init dicts\n",
    "        self.trainPositive = {}\n",
    "        self.trainNegative = {}\n",
    "        # counters\n",
    "        self.posGramCount = 0\n",
    "        self.negGramCount = 0\n",
    "        self.spamCount = 0\n",
    "        # priors\n",
    "        self.pA = 0\n",
    "        self.pNotA = 0\n",
    "        \n",
    "    def ngrams(self, n, text):\n",
    "        text = text.split(' ')\n",
    "        grams = []\n",
    "        for i in range(len(text)-n+1):\n",
    "            gram = ' '.join(text[i:i+n])\n",
    "            grams.append(gram)\n",
    "        return grams \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        for item in self.train_data:\n",
    "            label = item[0]\n",
    "            grams = item[1]\n",
    "            if label == 1:\n",
    "                self.spamCount += 1   \n",
    "            for gram in grams:\n",
    "                if label == 1:\n",
    "                    self.trainPositive[gram] = self.trainPositive.get(gram, 0) + 1\n",
    "                    self.posGramCount += 1\n",
    "                else:\n",
    "                    self.trainNegative[gram] = self.trainNegative.get(gram, 0) + 1\n",
    "                    self.negGramCount += 1\n",
    "                    \n",
    "        self.pA = self.spamCount/float(len(self.train_data))\n",
    "        self.pNotA = 1.0 - self.pA\n",
    "        \n",
    "    def classify(self, text, alpha=1.0):\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        isSpam = self.pA * self.conditionalText(text, 1)\n",
    "        notSpam = self.pNotA * self.conditionalText(text, 0)\n",
    "        if (isSpam > notSpam):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def conditionalText(self, grams, label):\n",
    "        result = 1.0\n",
    "        for ngram in grams:\n",
    "            result *= self.conditionalNgram(ngram, label)\n",
    "        return result\n",
    "    \n",
    "    def conditionalNgram(self, ngram, label):\n",
    "        alpha = self.alpha\n",
    "        if label == 1:\n",
    "            return ((self.trainPositive.get(ngram,0)+alpha) /\n",
    "                    float(self.posGramCount+alpha*self.unique))\n",
    "        else:\n",
    "            return ((self.trainNegative.get(ngram,0)+alpha) /\n",
    "                    float(self.negGramCount+alpha*self.unique))\n",
    "            \n",
    "    def evaluate_test_data(self):\n",
    "        results = []\n",
    "        for test in self.test_data:\n",
    "            label = test[0]\n",
    "            text = test[1]\n",
    "            ruling = self.classify(text)\n",
    "            if ruling == label:\n",
    "                results.append(1) \n",
    "            else:\n",
    "                results.append(0) \n",
    "                \n",
    "        print(\"Evaluated {} test cases. {:.2f}% Accuracy\".format(len(results), 100.0*sum(results)/float(len(results))))\n",
    "        return sum(results)/float(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_bayes = ngrams_bayes(data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_bayes.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1393 test cases. 96.48% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.964824120603015"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_bayes.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1393 test cases. 75.45% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7544867193108399"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_sms= ngrams_bayes(data,2) \n",
    "bigram_sms.train()\n",
    "bigram_sms.evaluate_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 1393 test cases. 48.17% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48169418521177315"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_sms = ngrams_bayes(data,3) \n",
    "trigram_sms.train()\n",
    "trigram_sms.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our bayesian classifier performs well with unigrams, ok with bigrams, and is basically guessing randomly when it comes to trigrams. Increasing the size of your grams does not help this classifier classify this dataset. This is likely due to the data being made of up of short messages with highly specific and colloquial words. Nearly none of the trigrams will occur more than once in this dataset. I imagine that larger ngrams used in a baysian classifer would work well with something like product reviews which are longer than text messages and use less colloquial language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well does a Naive Bayesian Classification System perform at distinguishing between lines from Biggie Smalls and 2Pac?\n",
    "![biggie-pac](./biggie-pac.jpg)\n",
    "Let's look at how well our classifier performs at telling the difference between East Coast and West Coast 90s hip hop, specifically with The Notorious BIG and 2Pac. \n",
    "\n",
    "I gathered this dataset myself by grabbing lyrics of off http://www.azlyrics.com and editing the lyrics text to only include verses by Biggie and 2Pac. In total I collected 32 songs, 16 from each. \n",
    "\n",
    "Below I preprocess the data to remove special characters, make everything lowercase, and I split each song up into lines and labeled the lines as either a Biggie line or a Pac line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biggie_df = pd.read_csv('./biggie_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "biggie_df.columns = [\"lyrics\"]\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "biggie_df[\"lyrics\"] = biggie_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>relax and take notes while i take tokes of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>good evenin ladies and gentlemen\\nhows everybo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>who shot ya\\nseperate the weak from the obsole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>when i die fuck it i wanna go to hell\\ncause i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>when the lala hits ya lyrics just splits ya\\nh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyrics\n",
       "11  relax and take notes while i take tokes of the...\n",
       "12  good evenin ladies and gentlemen\\nhows everybo...\n",
       "13  who shot ya\\nseperate the weak from the obsole...\n",
       "14  when i die fuck it i wanna go to hell\\ncause i...\n",
       "15  when the lala hits ya lyrics just splits ya\\nh..."
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biggie_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pac_df = pd.read_csv('./2pac_lyrics.csv', usecols=[1], encoding='latin-1', header=None)\n",
    "pac_df.columns = [\"lyrics\"]\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.replace('[^\\w\\s]','')\n",
    "pac_df[\"lyrics\"] = pac_df[\"lyrics\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>little something for my godson elijah\\nand a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yo mo bee mayn drop that shit\\nyou know what t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rest in peace to my motherfucker biggy smallz\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>makaveli in this killuminati\\nall through your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its just me against the world\\nnothin to lose\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics\n",
       "0  little something for my godson elijah\\nand a l...\n",
       "1  yo mo bee mayn drop that shit\\nyou know what t...\n",
       "2  rest in peace to my motherfucker biggy smallz\\...\n",
       "3  makaveli in this killuminati\\nall through your...\n",
       "4  its just me against the world\\nnothin to lose\\..."
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biggie_lyrics = biggie_df[\"lyrics\"].values\n",
    "biggie_lyrics = [ song.split('\\n') for song in biggie_lyrics]\n",
    "biggie_lyrics = [line for song in biggie_lyrics for line in song]\n",
    "pac_lyrics = pac_df[\"lyrics\"].values\n",
    "pac_lyrics = [ song.split('\\n') for song in pac_lyrics]\n",
    "pac_lyrics = [line for song in pac_lyrics for line in song]\n",
    "\n",
    "rap_lines = [] \n",
    "\n",
    "for line in biggie_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([0,str(line)]))\n",
    "        \n",
    "for line in pac_lyrics:\n",
    "    if len(line.split()) > 3:\n",
    "        rap_lines.append(np.array([1,str(line)]))\n",
    "        \n",
    "rap_lines = np.array(rap_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rap_lines = pd.DataFrame(rap_lines)\n",
    "rap_lines.columns = [\"label\",\"line\"]\n",
    "rap_lines.head()\n",
    "rap_lines['label'] = rap_lines['label'].replace(['0','1'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bayes_biggie_vs_pac = ngrams_bayes(rap_lines.values, 1, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bayes_biggie_vs_pac.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 197 test cases. 75.13% Accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.751269035532995"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayes_biggie_vs_pac.evaluate_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a small data set, let's run multiple trials with different train-test splits to get a better idea of what our average classification accuracy using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 197 test cases. 73.10% Accuracy\n",
      "Evaluated 197 test cases. 76.14% Accuracy\n",
      "Evaluated 197 test cases. 75.13% Accuracy\n",
      "Evaluated 197 test cases. 79.19% Accuracy\n",
      "Evaluated 197 test cases. 72.08% Accuracy\n",
      "Evaluated 197 test cases. 71.57% Accuracy\n",
      "Evaluated 197 test cases. 74.11% Accuracy\n",
      "Evaluated 197 test cases. 71.57% Accuracy\n",
      "Evaluated 197 test cases. 70.05% Accuracy\n",
      "Evaluated 197 test cases. 74.62% Accuracy\n",
      "Average Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for _ in range(10):\n",
    "    unigram = ngrams_bayes(rap_lines.values, 1, 0.9)\n",
    "    unigram.train()\n",
    "    results.append(unigram.evaluate_test_data())\n",
    "    \n",
    "print(\"Average Accuracy: {:.2f}\".format(sum(results)/float(len(results))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not bad, but how do bigram and trigram compare?\n",
    "\n",
    "##### Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 197 test cases. 68.02% Accuracy\n",
      "Evaluated 197 test cases. 69.04% Accuracy\n",
      "Evaluated 197 test cases. 69.54% Accuracy\n",
      "Evaluated 197 test cases. 73.10% Accuracy\n",
      "Evaluated 197 test cases. 72.59% Accuracy\n",
      "Evaluated 197 test cases. 72.08% Accuracy\n",
      "Evaluated 197 test cases. 74.62% Accuracy\n",
      "Evaluated 197 test cases. 66.50% Accuracy\n",
      "Evaluated 197 test cases. 71.57% Accuracy\n",
      "Evaluated 197 test cases. 64.97% Accuracy\n",
      "Average Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    bigram_net = ngrams_bayesnet(rap_lines.values, 2, 0.9)\n",
    "    bigram_net.train()\n",
    "    results.append(bigram_net.evaluate_test_data())\n",
    "    \n",
    "print(\"Average Accuracy: {:.2f}\".format(sum(results)/float(len(results))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 197 test cases. 71.07% Accuracy\n",
      "Evaluated 197 test cases. 71.07% Accuracy\n",
      "Evaluated 197 test cases. 70.05% Accuracy\n",
      "Evaluated 197 test cases. 71.57% Accuracy\n",
      "Evaluated 197 test cases. 70.56% Accuracy\n",
      "Evaluated 197 test cases. 72.08% Accuracy\n",
      "Evaluated 197 test cases. 72.08% Accuracy\n",
      "Evaluated 197 test cases. 64.47% Accuracy\n",
      "Evaluated 197 test cases. 71.07% Accuracy\n",
      "Evaluated 197 test cases. 64.47% Accuracy\n",
      "Average Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _ in range(10):\n",
    "    trigram_net = ngrams_bayesnet(rap_lines.values, 2, 0.9)\n",
    "    trigram_net.train()\n",
    "    results.append(trigram_net.evaluate_test_data())\n",
    "    \n",
    "print(\"Average Accuracy: {:.2f}\".format(sum(results)/float(len(results))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Unigrams seem to have a slight edge on bigrams and trigrams but using trigrams doesn't yeild horrible results on this rap data like it did when classing sms messages. My guess is that, while these raps contain highly colloquial words, there are phrases unique to Biggie and Pac that they use repeatedly while with sms messages the sequence of words is more arbitrary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
